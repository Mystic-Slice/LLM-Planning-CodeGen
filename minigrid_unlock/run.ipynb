{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f139a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from minigrid.wrappers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SEEDS = list(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniGrid-Unlock-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b513688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unwrapped(env):\n",
    "    while hasattr(env, 'env'):\n",
    "        env = env.env\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80239948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_obs(obs):\n",
    "    rgb_obs_env = RGBImgObsWrapper(env)\n",
    "\n",
    "    obs = rgb_obs_env.observation(obs)\n",
    "\n",
    "    plt.imshow(obs['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_obs(env):\n",
    "    obs = get_unwrapped(env).gen_obs()\n",
    "    fully_obs = FullyObsWrapper(env).observation(obs)\n",
    "    return fully_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(env):\n",
    "    # convert to np array\n",
    "    grid = env.get_wrapper_attr('pprint_grid')()\n",
    "    split_rows = grid.split('\\n')\n",
    "    grid_cells = [[line[i:i+2] for i in range(0, len(line), 2)] for line in split_rows]\n",
    "\n",
    "    OBJ_IDX_TO_TYPE = {\n",
    "        'W': \"WALL\",\n",
    "        'D': \"DOOR\",\n",
    "        'K': \"KEY\",\n",
    "        'A': \"BALL\",\n",
    "        'B': \"BOX\",\n",
    "        'L': \"DOOR\",\n",
    "        'V': 'AGENT',\n",
    "        '^': 'AGENT',\n",
    "        '>': 'AGENT',\n",
    "        '<': 'AGENT',\n",
    "        ' ': \"\",\n",
    "    }\n",
    "    input_grid = [[OBJ_IDX_TO_TYPE[c[0]] for c in row] for row in grid_cells]\n",
    "\n",
    "    direction = None\n",
    "    if 'V' in grid:\n",
    "        direction = 'DOWN'\n",
    "    elif '^' in grid:\n",
    "        direction = 'UP'\n",
    "    elif '<' in grid:\n",
    "        direction = 'LEFT'\n",
    "    elif '>' in grid:\n",
    "        direction = 'RIGHT'\n",
    "\n",
    "    return input_grid, direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_input_grid(grid):\n",
    "    OBJ_IDX_TO_TYPE = {\n",
    "        'W': \"WALL\",\n",
    "        'D': \"DOOR\",\n",
    "        'K': \"KEY\",\n",
    "        'A': \"BALL\",\n",
    "        'B': \"BOX\",\n",
    "        'L': \"DOOR\",\n",
    "        'V': 'AGENT',\n",
    "        '^': 'AGENT',\n",
    "        '>': 'AGENT',\n",
    "        '<': 'AGENT',\n",
    "    }\n",
    "    for row in grid:\n",
    "        print('[', end='')\n",
    "        print(\",\".join([f'\"{c}\"' for c in row]), end='],\\n')\n",
    "        # print('],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6eacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS_MAP = {\n",
    "    'LEFT': 0,\n",
    "    'RIGHT': 1,\n",
    "    'MOVE': 2,\n",
    "    'PICKUP': 3,\n",
    "    'DROP': 4,\n",
    "    'UNLOCK': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(env, actions):\n",
    "    action_ids = [ACTIONS_MAP[action] for action in actions]\n",
    "    # print(f\"Action IDs: {action_ids}\")\n",
    "\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    for action in action_ids:\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    return reward, done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_codes.o3_mini import solve as solve_o3_mini\n",
    "from model_codes_iterative.o3_mini_b import solve as solve_o3_mini_b\n",
    "from model_codes_iterative.o3_mini_c import solve as solve_o3_mini_c\n",
    "from model_codes_iterative.o3_mini_d import solve as solve_o3_mini_d\n",
    "\n",
    "from model_codes.claude import solve as solve_claude\n",
    "from model_codes_iterative.claude_b import solve as solve_claude_b\n",
    "from model_codes_iterative.claude_c import solve as solve_claude_c\n",
    "from model_codes_iterative.claude_d import solve as solve_claude_d\n",
    "from model_codes_iterative.claude_e import solve as solve_claude_e\n",
    "from model_codes_iterative.claude_f import solve as solve_claude_f\n",
    "from model_codes_iterative.claude_human_fix import solve as solve_claude_human_fix\n",
    "\n",
    "from model_codes.o1 import solve as solve_o1\n",
    "from model_codes_iterative.o1_b import solve as solve_o1_b\n",
    "from model_codes_iterative.o1_c import solve as solve_o1_c\n",
    "\n",
    "from model_codes.gemini_25_pro import solve as solve_gemini_25_pro\n",
    "from model_codes_iterative.gemini_25_pro_b import solve as solve_gemini_25_pro_b\n",
    "\n",
    "from model_codes.deepseek import solve as solve_deepseek\n",
    "from model_codes_iterative.deepseek_b import solve as solve_deepseek_b\n",
    "from model_codes_iterative.deepseek_c import solve as solve_deepseek_c\n",
    "from model_codes_iterative.deepseek_d import solve as solve_deepseek_d\n",
    "\n",
    "from model_codes.gpt_4o import solve as solve_gpt_4o\n",
    "from model_codes_iterative.gpt_4o_b import solve as solve_gpt_4o_b\n",
    "\n",
    "from model_codes.random_walk import solve as solve_random_walk\n",
    "from model_codes.greedy import solve as solve_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00467579",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = {\n",
    "    'baseline': {\n",
    "        'random': solve_random_walk,\n",
    "        'greedy': solve_greedy,\n",
    "    },\n",
    "    'direct_code_gen': {\n",
    "        'o3_mini': solve_o3_mini,\n",
    "        'claude': solve_claude,\n",
    "        'o1': solve_o1,\n",
    "        'gemini_25_pro': solve_gemini_25_pro,\n",
    "        'deepseek': solve_deepseek,\n",
    "        'gpt_4o': solve_gpt_4o,\n",
    "    },\n",
    "    'iterative': {\n",
    "        'o3_mini_b': solve_o3_mini_b,\n",
    "        'o3_mini_c': solve_o3_mini_c,\n",
    "        'o3_mini_d': solve_o3_mini_d,\n",
    "        'claude_b': solve_claude_b,\n",
    "        'claude_c': solve_claude_c,\n",
    "        'claude_d': solve_claude_d,\n",
    "        'claude_e': solve_claude_e,\n",
    "        'claude_f': solve_claude_f,\n",
    "        'o1_b': solve_o1_b,\n",
    "        'o1_c': solve_o1_c,\n",
    "        'gemini_25_pro_b': solve_gemini_25_pro_b,\n",
    "        'deepseek_b': solve_deepseek_b,\n",
    "        'deepseek_c': solve_deepseek_c,\n",
    "        'deepseek_d': solve_deepseek_d,\n",
    "        'gpt_4o_b': solve_gpt_4o_b,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_scores(scores, method_cls, method_name, grid_seed, reward, done):\n",
    "    if method_cls not in scores:\n",
    "        scores[method_cls] = {}\n",
    "    if method_name not in scores[method_cls]:\n",
    "        scores[method_cls][method_name] = {}\n",
    "    if grid_seed not in scores[method_cls][method_name]:\n",
    "        scores[method_cls][method_name][grid_seed] = {'reward': 0, 'done': 0}\n",
    "    \n",
    "    scores[method_cls][method_name][grid_seed]['reward'] = reward\n",
    "    scores[method_cls][method_name][grid_seed]['done'] = 1 if done else 0\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(scores, return_df = False):\n",
    "    metrics = {}\n",
    "    data_points = []\n",
    "    for method_cls, method_scores in scores.items():\n",
    "        metrics[method_cls] = {}\n",
    "        for method_name, grid_scores in method_scores.items():\n",
    "            total_reward = sum([grid_score['reward'] for grid_score in grid_scores.values()])\n",
    "            total_done = sum([grid_score['done'] for grid_score in grid_scores.values()])\n",
    "            num_grids = len(grid_scores)\n",
    "            metrics[method_cls][method_name] = {\n",
    "                'total_reward': total_reward,\n",
    "                'total_done': total_done,\n",
    "                'num_grids': num_grids,\n",
    "                'avg_reward': total_reward / num_grids if num_grids > 0 else 0,\n",
    "                'avg_done': total_done / num_grids if num_grids > 0 else 0,\n",
    "            }\n",
    "\n",
    "            for grid_seed, grid_score in grid_scores.items():\n",
    "                data_points.append({\n",
    "                    'method_cls': method_cls,\n",
    "                    'method_name': method_name,\n",
    "                    'grid_seed': grid_seed,\n",
    "                    'reward': grid_score['reward'],\n",
    "                    'done': grid_score['done'],\n",
    "                })\n",
    "\n",
    "    if return_df:\n",
    "        df = pd.DataFrame(data_points)\n",
    "        return metrics, df\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_grids(scores, method_cls, method_name, k=3):\n",
    "    grid_scores = scores[method_cls][method_name]\n",
    "    not_done_grids = [(k, v) for k, v in grid_scores.items() if v['done'] == 0]\n",
    "    \n",
    "    worst_grids = not_done_grids\n",
    "\n",
    "    done_grids = {k: v for k, v in grid_scores.items() if v['done'] == 1}\n",
    "    done_grids = sorted(done_grids.items(), key=lambda x: x[1]['reward'], reverse=True)\n",
    "    worst_grids += done_grids\n",
    "    return worst_grids[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df638d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "\n",
    "for method_cls, methods_dict in METHODS.items():\n",
    "    for method_name, method in methods_dict.items():\n",
    "        print(f\"Method: {method_name}\")\n",
    "        for grid_seed in GRID_SEEDS:\n",
    "            print(f\"Grid Seed: {grid_seed}\\r\", end=\"\")\n",
    "\n",
    "            env.reset(seed=grid_seed)\n",
    "            input_grid, direction = get_inputs(env)\n",
    "\n",
    "            actions = method(input_grid, direction)\n",
    "            # print(f\"Actions: {actions}\")\n",
    "\n",
    "            reward, done = eval(env, actions)\n",
    "            # print(f\"Done: {done}\")\n",
    "            # print(f\"Reward: {reward}\")\n",
    "            \n",
    "            scores = add_to_scores(scores, method_cls, method_name, grid_seed, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e22c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_metrics(scores)\n",
    "for method_cls, method_scores in metrics.items():\n",
    "    print(f\"Method Class: {method_cls}\")\n",
    "    for method_name, s in method_scores.items():\n",
    "        print(f\"  Method Name: {method_name}\")\n",
    "        print(f\"    Average Reward: {s['avg_reward']}\")\n",
    "        print(f\"    Average Done: {s['avg_done']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, metrics_df = get_metrics(scores, return_df=True)\n",
    "metrics_df.to_csv('metrics_unlock.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e22f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bad_grid_seed, results in get_worst_grids(scores, 'iterative', 'gpt_4o_b'):\n",
    "    env.reset(seed=bad_grid_seed)\n",
    "    input_grid, direction = get_inputs(env)\n",
    "\n",
    "    # print(f\"Grid Seed: {bad_grid_seed}\")\n",
    "    # print(results)\n",
    "    \n",
    "    print(\"<grid>\")\n",
    "    print_input_grid(input_grid)\n",
    "    print(\"</grid>\")\n",
    "    print(f\"<start_direction>\\n{direction}\\n</start_direction>\")\n",
    "    print(\"--------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc7a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
